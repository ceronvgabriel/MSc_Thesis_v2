{"train_acc_v": [19.286, 34.638, 42.326, 46.914, 49.88], "train_loss_v": [3.1341906550216674, 1.7520610162734986, 1.5593849576950074, 1.4414087252426147, 1.3687255792999267], "test_acc_v": [28.37, 41.03, 45.55, 50.65, 51.13], "test_loss_v": [1.9071842300891877, 1.5742775666713715, 1.489610468149185, 1.3685689520835878, 1.3422311079502105], "current_lr_v": [[0.09045084971874738], [0.06545084971874737], [0.03454915028125263], [0.009549150281252633], [0.0]], "parameters": "{'current_function': 'progressive_train_3', 'batch_size': 1024, 'n_workers': 2, 'optimizer': {'class': <class 'torch.optim.sgd.SGD'>, 'dict': {'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.0005, 'nesterov': False}}, 'scheduler': {'class': <class 'torch.optim.lr_scheduler.CosineAnnealingLR'>, 'dict': {'T_max': 5, 'eta_min': 0, 'optimizer': SGD (\nParameter Group 0\n    dampening: 0\n    initial_lr: 0.1\n    lr: 0.0\n    momentum: 0.9\n    nesterov: False\n    weight_decay: 0.0005\n), 'base_lrs': [0.1], 'last_epoch': 5, '_step_count': 6, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.0]}}}", "tot_time": 83.811046859}