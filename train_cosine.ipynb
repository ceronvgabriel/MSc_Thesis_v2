{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%run imports.py"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Files already downloaded and verified\nPreparing data..\nFiles already downloaded and verified\nGPU available:  True\nOS:  linux\n"
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#bss=[64,128,256,512,1024]\r\n",
        "bss=[128]\r\n",
        "sch=\"cosine\"\r\n",
        "total_epochs=160\r\n",
        "step=40\r\n",
        "\r\n",
        "def train_prog():\r\n",
        "    for bs in bss:\r\n",
        "        print(\"Training bs: \" + str(bs))\r\n",
        "        name=\"bs_\"+str(bs) + \"_sch_\"+ sch\r\n",
        "        save_folder=name+\"/\"+name\r\n",
        "\r\n",
        "        net=models.resnet18(num_classes=10)\r\n",
        "        criterion = nn.CrossEntropyLoss()\r\n",
        "        # Observe that all parameters are being optimized\r\n",
        "        optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\r\n",
        "        # Decay LR by a factor of 0.1 every 7 epochs\r\n",
        "        # exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\r\n",
        "        #plateau_lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=3,threshold=1e-3)\r\n",
        "        if sch==\"cosine\":\r\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=150)\r\n",
        "        elif sch==\"plateau\":\r\n",
        "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=3,threshold=1e-3)\r\n",
        "\r\n",
        "        model_actions.progressive_train_4(net,epochs=total_epochs,step=step,tr_bs=bs,save_folder=save_folder,criterion=criterion,scheduler=scheduler)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "az_manage_proc.run_and_delete(log.log_time,train_prog)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#vary T_max for cosine scheduler\r\n",
        "tmaxs=[40,80,160]\r\n",
        "sch=\"cosine\"\r\n",
        "total_epochs=160\r\n",
        "step=40\r\n",
        "bs=128\r\n",
        "\r\n",
        "def train_prog():\r\n",
        "    for tmax in tmaxs:\r\n",
        "        print(\"Training bs: \" + str(bs))\r\n",
        "        name=\"bs_\"+str(bs) + \"_sch_\"+ sch + \"_tmax_\" + str(tmax)\r\n",
        "        save_folder=name+\"/\"+name\r\n",
        "\r\n",
        "        net=models.resnet18(num_classes=10)\r\n",
        "        criterion = nn.CrossEntropyLoss()\r\n",
        "        # Observe that all parameters are being optimized\r\n",
        "        optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\r\n",
        "        # Decay LR by a factor of 0.1 every 7 epochs\r\n",
        "        # exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\r\n",
        "        #plateau_lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=3,threshold=1e-3)\r\n",
        "        if sch==\"cosine\":\r\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=tmax)\r\n",
        "        elif sch==\"plateau\":\r\n",
        "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=3,threshold=1e-3)\r\n",
        "\r\n",
        "        model_actions.progressive_train_4(net,epochs=total_epochs,step=step,tr_bs=bs,save_folder=save_folder,criterion=criterion,scheduler=scheduler)"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1634184305338
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "az_manage_proc.run_and_delete(log.log_time,train_prog)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found existing instance, use it.\nLogging time\nTraining bs: 128\nTraining\n\n\nEpoch: 1\nSaving best..\n\nEpoch: 2\nSaving best..\n\nEpoch: 3\nSaving best..\n\nEpoch: 4\nSaving best..\n\nEpoch: 5\nSaving best..\n\nEpoch: 6\nSaving best..\n\nEpoch: 7\n\nEpoch: 8\nSaving best..\n\nEpoch: 9\n\nEpoch: 10\n\nEpoch: 11\nSaving best..\n\nEpoch: 12\n\nEpoch: 13\nSaving best..\n\nEpoch: 14\n\nEpoch: 15\n\nEpoch: 16\nSaving best..\n\nEpoch: 17\nSaving best..\n\nEpoch: 18\n\nEpoch: 19\n\nEpoch: 20\nSaving best..\n\nEpoch: 21\nSaving best..\n\nEpoch: 22\nSaving best..\n\nEpoch: 23\n\nEpoch: 24\nSaving best..\n\nEpoch: 25\n\nEpoch: 26\n\nEpoch: 27\nSaving best..\n\nEpoch: 28\n\nEpoch: 29\nSaving best..\n\nEpoch: 30\nSaving best..\n\nEpoch: 31\nSaving best..\n\nEpoch: 32\nSaving best..\n\nEpoch: 33\nSaving best..\n\nEpoch: 34\nSaving best..\n\nEpoch: 35\nSaving best..\n\nEpoch: 36\nSaving best..\n\nEpoch: 37\nSaving best..\n\nEpoch: 38\nSaving best..\n\nEpoch: 39\nSaving best..\n\nEpoch: 40\nSaving step..\n\nEpoch: 41\n\nEpoch: 42\nSaving best..\n\nEpoch: 43\n\nEpoch: 44\n\nEpoch: 45\n\nEpoch: 46\n\nEpoch: 47\n\nEpoch: 48\n\nEpoch: 49\n\nEpoch: 50\n\nEpoch: 51\n\nEpoch: 52\n\nEpoch: 53\n\nEpoch: 54\n\nEpoch: 55\n\nEpoch: 56\n\nEpoch: 57\n\nEpoch: 58\n\nEpoch: 59\n\nEpoch: 60\n\nEpoch: 61\n\nEpoch: 62\n\nEpoch: 63\n\nEpoch: 64\n\nEpoch: 65\n\nEpoch: 66\n\nEpoch: 67\n\nEpoch: 68\n\nEpoch: 69\n\nEpoch: 70\n\nEpoch: 71\n\nEpoch: 72\n\nEpoch: 73\n\nEpoch: 74\n\nEpoch: 75\n\nEpoch: 76\n\nEpoch: 77\n\nEpoch: 78\n\nEpoch: 79\n\nEpoch: 80\nSaving step..\n\nEpoch: 81\n\nEpoch: 82\n\nEpoch: 83\n\nEpoch: 84\n\nEpoch: 85\n\nEpoch: 86\n\nEpoch: 87\n\nEpoch: 88\n\nEpoch: 89\n\nEpoch: 90\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python3-azureml"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}